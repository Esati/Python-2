{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLTK (Natural Language Toolkit)\n",
    "\n",
    "NLTK offers a comprehensive set of tools for Natural Language Processing (NLP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Corpora\n",
    "\n",
    "A corpus (plural corpora) is a sample of real world text used in NLP.\n",
    "\n",
    "We will illustrate various tools contained in NLTK using the `reuters` corpus. It contains newswire articles from Reuters from 1987. To work with an NLTK corpus, we need to download it.\n",
    "\n",
    "This is just a sample of a real world text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import re\n",
    "import string\n",
    "nltk.download('reuters')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will need some other NLTK packages later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Reuters corpus is quite large. For our purposes of illustrating the use of NLTK, we will make it smaller. \n",
    "\n",
    "The `words` method returns the number of words contained in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "len(reuters.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The files contained in the corpus are assigned to categories. We will reduce the size of the data by choosing only two of the categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "reuters.categories()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "reuters_use = reuters.words(categories=['cocoa','coffee'])\n",
    "len(reuters_use)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Stopwords\n",
    "\n",
    "Stopwords are very frequent words that usually don't contribute much when trying to categorize text in terms of content.\n",
    "\n",
    "If we are only concerned with the content, we should remove them. If we care about the style of the language used in the text, they can be useful and should be retained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Lower case\n",
    "It is usually a good idea to convert all text to lower case. We also remove the stopwords here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "reuters_use = [w.lower() for w in reuters_use if w.lower() not in stopwords.words('english')]\n",
    "reuters_use[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Unless we are interested in using the structure of sentences, we remove the punctuation from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We also need to create punctuation sequences so that we can eliminate them from the text as otherwise only individual punctuation characters are removed. Such sequences usually contain a quotation mark, which is why we focus on those here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "reuters_use = [w for w in reuters_use if w not in string.punctuation]\n",
    "punct_seq = [c+\"\\\"\" for c in string.punctuation ]+ [\"\\\"\"+c for c in string.punctuation ]\n",
    "reuters_use = [w for w in reuters_use if w not in punct_seq]\n",
    "len(reuters_use)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Uni-grams, bi-grams, n-grams\n",
    "In NLP, sequences of words of length `n` are called `n-grams`. Individual words are called uni-grams, sequences of two words bi-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Frequency distribution of words\n",
    "We can create a `FreqDist` object that provides the number of times the distinct words occur in the text, beginning with the most frequent word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "freq_dist_uni = nltk.FreqDist(reuters_use)\n",
    "freq_dist_uni.plot(30, cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To see how many times the most frequent words occurred, we can also use the `most_common` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for word, frequency in freq_dist_uni.most_common(10):\n",
    "    print(word, frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Frequency distribution of bi-grams\n",
    "\n",
    "The `bigrams` function returns a generator over the bi-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "reuters_use_bi = nltk.bigrams(reuters_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "freq_dist_bi = nltk.FreqDist(reuters_use_bi)\n",
    "freq_dist_bi.plot(30, cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can again use the `most_common` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for bigram, frequency in freq_dist_bi.most_common(10):\n",
    "    print(bigram, frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Tokenization\n",
    "\n",
    "Tokenization refers to the separation of a text into smaller units. Sentence tokenization splits the text into sentences, word tokenization into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "my_text = \"\"\"\n",
    "For the first quarter, Fifth Third saw an increase in their net interest income YOY (NII) to $132 million,\n",
    "but there was a decrease in the net interest margin of 4 basis points. These increases were driven by the\n",
    "impacts of the MB Financial merger, as well as successful cash flow hedges. The decrease in NIM of 4 bps\n",
    "was primarily from the challenging interest rate environment, of which the Fed dropped the fed funds rate\n",
    "from 1.25% to the current level of 0 to 0.25%.\n",
    "The decline in interest rates will have a big impact on the NIM in the coming quarters, more on that in\n",
    "a moment.\n",
    "Noninterest income also saw increases in YOY, $165 million, or 29%, which included an impact from the MB\n",
    "Financial merger. The increase was driven by increases in the mortgage revenue increases of 114%,\n",
    "leasing revenue up 128% primarily from the MB Financial merger, Wealth and asset management revenue up 20%.\n",
    "On an adjusted basis, both return on assets 1.19%, and return on equity 9.9%, and an efficiency ratio of\n",
    "59.4% were all within line and according to estimates.\n",
    "And dividend payments of $1.08 annually with a current yield of 6.22%, per Seeking Alpha.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Sentence Tokenization\n",
    "The default sentence tokenizer in NLTK is the `PunktSentenceTokenizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(my_text.replace('\\n',' ').strip())\n",
    "for s in sentences:\n",
    "    print(s+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This worked well, though it can be less than perfect if a text contains non-standard punctuation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Word Tokenization\n",
    "There are several word tokenizers available. We will look at three common ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sentence = \"And dividend payments of $1.08 annually with a current yield of 6.22%, that's a lot, ain't it?\"\n",
    "\n",
    "default_tokens = word_tokenize(sentence)   # nltk.download('punkt') for this\n",
    "\n",
    "punct_tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "punct_tokens = punct_tokenizer.tokenize(sentence)\n",
    "\n",
    "space_tokenizer = nltk.tokenize.SpaceTokenizer()\n",
    "space_tokens = space_tokenizer.tokenize(sentence)\n",
    "\n",
    "print(default_tokens, '\\n')\n",
    "print(punct_tokens, '\\n')\n",
    "print(space_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### `Text` objects\n",
    "You can create NLTK Text objects which then allow you to apply the NLTK methods to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "my_tokenized_text = word_tokenize(my_text)\n",
    "my_nltk_text     = nltk.Text(my_tokenized_text)\n",
    "\n",
    "freq_my_nltk_text = nltk.FreqDist(my_nltk_text)\n",
    "freq_my_nltk_text.plot(10, cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Part of Speech Tagging\n",
    "\n",
    "If we are concerned not only with the words we observe but also with their grammatical categories, we need Part of Speech (POS) tagging. A POS-tagger can be used to classify each word.\n",
    "\n",
    "As for the tokenizers, there exist multiple POS-taggers, though we will consider only the default one here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "default_pos = nltk.pos_tag(default_tokens)\n",
    "print(default_pos)\n",
    "punct_pos = nltk.pos_tag(punct_tokens)\n",
    "print(punct_pos)\n",
    "space_pos = nltk.pos_tag(space_tokens)\n",
    "print(space_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You can find a definition of each tag at \n",
    "\n",
    "https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can use regular expressions to extract larger classes of tags. E.g., all noun categories begin with an 'N'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "regex = re.compile('^N.*')\n",
    "nouns = []\n",
    "for l in default_pos:\n",
    "    if regex.match(l[1]):\n",
    "        nouns.append(l[0])\n",
    "print(\"Nouns:\", nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Stemming\n",
    "Stemming is the process of removing the suffixes from words. As for some of the other tools we have considered, there are multiple stemmers available. We take a look at what some of them do here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "snowball = nltk.stem.snowball.SnowballStemmer('english')\n",
    "\n",
    "for stemmer in [porter, lancaster, snowball]:\n",
    "    print([stemmer.stem(t) for t in default_tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Lemmatizing\n",
    "\n",
    "Lemmatization is similar to stemming in that it eliminates suffixes, but its goal is to find the root word you would find in a dictionary instead of merely a truncated version of the word.  \n",
    "\n",
    "There are again multiple choices available in NLTK. The default is the `Wordnet` lemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "wordnet = nltk.WordNetLemmatizer()\n",
    "print([wordnet.lemmatize(t) for t in default_tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Concordance\n",
    "Concordance returns the context in which a word appears in the text. You can specify the `width`, i.e., the number of characters to be displayed, by passing it as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(nltk.Text(reuters_use).concordance('beans', width = 70 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This has only been a very brief overview of NLTK. There is much more in the book at https://www.nltk.org/book/ and in NLTK's documentation."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
